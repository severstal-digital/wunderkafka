{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Wunderkafka's documentation!","text":"<p>The power of librdkafka for humans pythons</p> <p>Wunderkafka provides handful facade for C-powered consumer/producer. It's built on top of the confluent-kafka.</p>"},{"location":"#rationale","title":"Rationale","text":"<pre><code>Das ist wunderbar!\n</code></pre>"},{"location":"#what-we-are-about","title":"What we are about?","text":"<ul> <li>Cloudera installation with its own     schema registry</li> <li>Apache Avro\u2122 is used</li> <li>Installation requires features which are fully supported by     librdkafka, but not     bundled in confluent-kafka python wheel</li> <li>Constant need to use producers and consumer, but without one-screen     boilerplate</li> <li>Frequent need to consume not purely events, but fairly recent     events</li> <li>Frequent need to handle a large number of events</li> </ul> <p>So, that's it.</p> <p>If you suffer from the same problems, you may don't need to reinvent your own wheel, you can try ours.</p>"},{"location":"#what-about-other-projects","title":"What about other projects?","text":"<p>Corresponding to ASF wiki there are plenty of python clients.</p> <ul> <li>confluent-kafka is a     de-facto standard, but doesn't work out-of-the-box for us, as     mentioned above</li> <li>Kafka Python is awesome, but     not as performant as confluent-kafka</li> <li>pykafka here and     here looks unmaintained: has     been archived</li> <li>pykafkap has only     producer and looks unmaintained: no updates since 2014</li> <li>brod is not maintained in favor     of Kafka Python.</li> </ul>"},{"location":"#whats-next","title":"What's next?","text":"<p>For now, it's a homebrew, so it lacks some of the features which may be useful outside of our use-cases.</p> <p>ToDo:</p> <ul> <li>add configurations for multiple versions of librdkafka</li> <li>check against confluent installation</li> <li>add <code>async</code>/<code>await</code> syntax</li> <li>parallelize (de)serialization on CPU</li> <li>add distributed lock on producers</li> <li>add on-the-fly model derivation to consumer</li> <li>???</li> </ul>"},{"location":"#errata","title":"Errata","text":""},{"location":"#kerberos-thread","title":"Kerberos Thread","text":""},{"location":"#version-librdkafka-170","title":"Version librdkafka &lt; 1.7.0","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Prior to version 1.7.0, updating kinit via librdkafka could result in a lockup. Because of this, when using a version less than 1.7.0, by default used our thread to update kerberos tickets.</p>"},{"location":"#standard-behavior","title":"Standard behavior","text":"<p>By default, 1 thread is raised to update all tickets and the timeout for all updates is 60 seconds.</p> <p>To set timeout manually you need to</p> <pre><code>from wunderkafka.hotfixes.watchdog import KrbWatchDog\n\nif __name__ == '__main__':\n    krb = KrbWatchDog()\n    krb.krb_timeout = 10  # complete\n    krb.krb_timeout = 'something'  # raised TypeError\n</code></pre>"},{"location":"#bottleneck","title":"Bottleneck","text":"<p>Updating of kerberos tickets is done sequentially one by one.</p>"},{"location":"pages/API/","title":"API","text":""},{"location":"pages/API/#raw-bytes","title":"Raw Bytes","text":""},{"location":"pages/API/#bytesconsumer","title":"BytesConsumer","text":"<p>               Bases: <code>AbstractConsumer</code></p> <p>Consumer implementation of extended interface for raw messages.</p> Source code in <code>wunderkafka/consumers/bytes.py</code> <pre><code>class BytesConsumer(AbstractConsumer):\n    \"\"\"Consumer implementation of extended interface for raw messages.\"\"\"\n\n    # FixMe (tribunsky.kir): add watchdog page reference\n    def __init__(self, config: ConsumerConfig, sasl_watchdog: Optional[Watchdog] = None) -&gt; None:\n        \"\"\"\n        Init consumer.\n\n        :param config:          Pydantic BaseSettings model with librdkafka consumer's configuration.\n        :param sasl_watchdog:   Callable to handle the global state of kerberos auth (see Watchdog).\n        \"\"\"\n        try:\n            super().__init__(config.dict())\n        except KafkaException as exc:\n            config = challenge_krb_arg(exc, config)\n            super().__init__(config.dict())\n        self.subscription_offsets: Optional[Dict[str, HowToSubscribe]] = None\n\n        self._config = config\n        self._last_poll_ts = time.perf_counter()\n        self._sasl_watchdog = sasl_watchdog\n        # ToDo (tribunsky-kir): make it configurable\n        atexit.register(self.close)\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Get human-readable representation of consumer.\n\n        :return:    string with consumer gid.\n        \"\"\"\n        return '{0}:{1}'.format(self.__class__.__name__, self._config.group_id)\n\n    def batch_poll(  # noqa: D102 # inherited from superclass.\n        self,\n        timeout: float = 1.0,\n        num_messages: int = 1000000,\n        *,\n        raise_on_lost: bool = False,\n    ) -&gt; List[Message]:\n        if self._sasl_watchdog is not None:\n            self._sasl_watchdog()\n\n        # ToDo (tribunsky.kir): naybe it better to use on_lost callback within subscribe()\n        dt = int((time.perf_counter() - self._last_poll_ts) * 1000)\n        if dt &gt; self._config.max_poll_interval_ms:\n            msg = 'Exceeded max.poll.interval.ms ({0}): {1}'.format(self._config.max_poll_interval_ms, dt)\n\n            if raise_on_lost:\n                # ToDo (tribunsky.kir): resubscribe by ts?\n                raise ConsumerException(msg)\n            logger.warning(msg)\n\n        msgs = self.consume(num_messages=num_messages, timeout=timeout)\n        self._last_poll_ts = time.perf_counter()\n        return msgs\n\n    # ToDo (tribunsky.kir): do not override original API and wrap it in superclass\n    def subscribe(  # noqa: D102,WPS211  # inherited from superclass.\n        self,\n        topics: List[Union[str, TopicSubscription]],\n        *,\n        from_beginning: Optional[bool] = None,\n        offset: Optional[int] = None,\n        ts: Optional[int] = None,\n        with_timedelta: Optional[datetime.timedelta] = None,\n    ) -&gt; None:\n        \"\"\"\n        Subscribe to a list of topics, or a list of specific TopicSubscriptions.\n\n        This method overrides original `subscribe()` method of `confluent-kafka.Consumer` and allows to subscribe\n        to topic via specific offset or timestamp.\n\n        .. warning::\n            Currently this method doesn't allow to pass callbacks and uses it's own to reset partitions.\n        \"\"\"  # noqa: E501\n        subscriptions = {}\n        for tpc in topics:\n            if isinstance(tpc, str):\n                tpc = TopicSubscription(\n                    topic=tpc, from_beginning=from_beginning, offset=offset, ts=ts, with_timedelta=with_timedelta,\n                )\n            subscriptions[tpc.topic] = tpc.how\n\n        # We have a specific subscription at least once\n        if any(subscriptions.values()):\n            self.subscription_offsets = subscriptions\n            # ToDo (tribunsky.kir): avoid mutation of self.subscription_offset and remove it as a field\n            super().subscribe(topics=list(self.subscription_offsets), on_assign=reset_partitions)\n        else:\n            super().subscribe(topics=list(subscriptions))\n</code></pre>"},{"location":"pages/API/#wunderkafka.BytesConsumer.__init__","title":"<code>__init__(config, sasl_watchdog=None)</code>","text":"<p>Init consumer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Pydantic BaseSettings model with librdkafka consumer's configuration.</p> required <code>sasl_watchdog</code> <code>Optional[Watchdog]</code> <p>Callable to handle the global state of kerberos auth (see Watchdog).</p> <code>None</code> Source code in <code>wunderkafka/consumers/bytes.py</code> <pre><code>def __init__(self, config: ConsumerConfig, sasl_watchdog: Optional[Watchdog] = None) -&gt; None:\n    \"\"\"\n    Init consumer.\n\n    :param config:          Pydantic BaseSettings model with librdkafka consumer's configuration.\n    :param sasl_watchdog:   Callable to handle the global state of kerberos auth (see Watchdog).\n    \"\"\"\n    try:\n        super().__init__(config.dict())\n    except KafkaException as exc:\n        config = challenge_krb_arg(exc, config)\n        super().__init__(config.dict())\n    self.subscription_offsets: Optional[Dict[str, HowToSubscribe]] = None\n\n    self._config = config\n    self._last_poll_ts = time.perf_counter()\n    self._sasl_watchdog = sasl_watchdog\n    # ToDo (tribunsky-kir): make it configurable\n    atexit.register(self.close)\n</code></pre>"},{"location":"pages/API/#wunderkafka.BytesConsumer.__str__","title":"<code>__str__()</code>","text":"<p>Get human-readable representation of consumer.</p> <p>Returns:</p> Type Description <code>str</code> <p>string with consumer gid.</p> Source code in <code>wunderkafka/consumers/bytes.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Get human-readable representation of consumer.\n\n    :return:    string with consumer gid.\n    \"\"\"\n    return '{0}:{1}'.format(self.__class__.__name__, self._config.group_id)\n</code></pre>"},{"location":"pages/API/#wunderkafka.BytesConsumer.subscribe","title":"<code>subscribe(topics, *, from_beginning=None, offset=None, ts=None, with_timedelta=None)</code>","text":"<p>Subscribe to a list of topics, or a list of specific TopicSubscriptions.</p> <p>This method overrides original <code>subscribe()</code> method of <code>confluent-kafka.Consumer</code> and allows to subscribe to topic via specific offset or timestamp.</p> <p>.. warning::     Currently this method doesn't allow to pass callbacks and uses it's own to reset partitions.</p> Source code in <code>wunderkafka/consumers/bytes.py</code> <pre><code>def subscribe(  # noqa: D102,WPS211  # inherited from superclass.\n    self,\n    topics: List[Union[str, TopicSubscription]],\n    *,\n    from_beginning: Optional[bool] = None,\n    offset: Optional[int] = None,\n    ts: Optional[int] = None,\n    with_timedelta: Optional[datetime.timedelta] = None,\n) -&gt; None:\n    \"\"\"\n    Subscribe to a list of topics, or a list of specific TopicSubscriptions.\n\n    This method overrides original `subscribe()` method of `confluent-kafka.Consumer` and allows to subscribe\n    to topic via specific offset or timestamp.\n\n    .. warning::\n        Currently this method doesn't allow to pass callbacks and uses it's own to reset partitions.\n    \"\"\"  # noqa: E501\n    subscriptions = {}\n    for tpc in topics:\n        if isinstance(tpc, str):\n            tpc = TopicSubscription(\n                topic=tpc, from_beginning=from_beginning, offset=offset, ts=ts, with_timedelta=with_timedelta,\n            )\n        subscriptions[tpc.topic] = tpc.how\n\n    # We have a specific subscription at least once\n    if any(subscriptions.values()):\n        self.subscription_offsets = subscriptions\n        # ToDo (tribunsky.kir): avoid mutation of self.subscription_offset and remove it as a field\n        super().subscribe(topics=list(self.subscription_offsets), on_assign=reset_partitions)\n    else:\n        super().subscribe(topics=list(subscriptions))\n</code></pre>"},{"location":"pages/API/#bytesproducer","title":"BytesProducer","text":"<p>               Bases: <code>AbstractProducer</code></p> <p>Producer implementation of extended interface for raw messages.</p> Source code in <code>wunderkafka/producers/bytes.py</code> <pre><code>class BytesProducer(AbstractProducer):\n    \"\"\"Producer implementation of extended interface for raw messages.\"\"\"\n\n    # FixMe (tribunsky.kir): add watchdog page reference\n    def __init__(self, config: ProducerConfig, sasl_watchdog: Optional[Watchdog] = None) -&gt; None:\n        \"\"\"\n        Init producer.\n\n        :param config:          Pydantic model with librdkafka producer's configuration.\n        :param sasl_watchdog:   Callable to handle global state of kerberos auth (see Watchdog).\n        \"\"\"\n        try:\n            super().__init__(config.dict())\n        except KafkaException as exc:\n            config = challenge_krb_arg(exc, config)\n            super().__init__(config.dict())\n\n        self._config = config\n        self._sasl_watchdog = sasl_watchdog\n        atexit.register(self.flush)\n\n    # ToDo (tribunsky.kir): make inherited from RDConfig models immutable.\n    #                       Currently it explodes because of mutation in watchdog.\n    #                       Do we need re-initiation of consumer/producer in runtime?\n    @property\n    def config(self) -&gt; ProducerConfig:\n        \"\"\"\n        Get the producer's config.\n\n        :return:        Pydantic model with librdkafka producer's configuration.\n        \"\"\"\n        return self._config\n\n    def send_message(  # noqa: D102,WPS211  # inherited from superclass.\n        self,\n        topic: str,\n        value: Optional[Union[str, bytes]] = None,  # noqa: WPS110  # Domain. inherited from superclass.\n        key: Optional[Union[str, bytes]] = None,\n        partition: Optional[int] = None,\n        on_delivery: Optional[DeliveryCallback] = error_callback,\n        *args: Any,\n        blocking: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        if self._sasl_watchdog is not None:\n            self._sasl_watchdog()\n        if partition is not None:\n            self.produce(topic, value, key=key, partition=partition, on_delivery=on_delivery, **kwargs)\n        else:\n            self.produce(topic, value, key=key, on_delivery=on_delivery, **kwargs)\n        if blocking:\n            self.flush()\n        else:\n            self.poll(0)\n</code></pre>"},{"location":"pages/API/#wunderkafka.BytesProducer.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get the producer's config.</p> <p>Returns:</p> Type Description <code>ProducerConfig</code> <p>Pydantic model with librdkafka producer's configuration.</p>"},{"location":"pages/API/#wunderkafka.BytesProducer.__init__","title":"<code>__init__(config, sasl_watchdog=None)</code>","text":"<p>Init producer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ProducerConfig</code> <p>Pydantic model with librdkafka producer's configuration.</p> required <code>sasl_watchdog</code> <code>Optional[Watchdog]</code> <p>Callable to handle global state of kerberos auth (see Watchdog).</p> <code>None</code> Source code in <code>wunderkafka/producers/bytes.py</code> <pre><code>def __init__(self, config: ProducerConfig, sasl_watchdog: Optional[Watchdog] = None) -&gt; None:\n    \"\"\"\n    Init producer.\n\n    :param config:          Pydantic model with librdkafka producer's configuration.\n    :param sasl_watchdog:   Callable to handle global state of kerberos auth (see Watchdog).\n    \"\"\"\n    try:\n        super().__init__(config.dict())\n    except KafkaException as exc:\n        config = challenge_krb_arg(exc, config)\n        super().__init__(config.dict())\n\n    self._config = config\n    self._sasl_watchdog = sasl_watchdog\n    atexit.register(self.flush)\n</code></pre>"},{"location":"pages/API/#schemaless","title":"Schemaless","text":""},{"location":"pages/API/#schemalessjsonstringconsumer","title":"SchemaLessJSONStringConsumer","text":"<p>               Bases: <code>HighLevelDeserializingConsumer</code></p> <p>Kafka Consumer client to get JSON-serialized messages without any schema.</p> Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>class SchemaLessJSONStringConsumer(HighLevelDeserializingConsumer):\n    \"\"\"Kafka Consumer client to get JSON-serialized messages without any schema.\"\"\"\n\n    def __init__(self, config: ConsumerConfig) -&gt; None:\n        \"\"\"\n        Init consumer from pre-defined blocks.\n\n        :param config:      Configuration for:\n\n                                - Librdkafka consumer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n        \"\"\"\n\n        self._default_timeout: int = 60\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            consumer=BytesConsumer(config, watchdog),\n            schema_registry=None,\n            headers_handler=None,\n            value_deserializer=SchemaLessJSONDeserializer(),\n            key_deserializer=StringDeserializer(),\n            stream_result=False,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.schemaless.SchemaLessJSONStringConsumer.__init__","title":"<code>__init__(config)</code>","text":"<p>Init consumer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Configuration for:  - Librdkafka consumer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>def __init__(self, config: ConsumerConfig) -&gt; None:\n    \"\"\"\n    Init consumer from pre-defined blocks.\n\n    :param config:      Configuration for:\n\n                            - Librdkafka consumer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n    \"\"\"\n\n    self._default_timeout: int = 60\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        consumer=BytesConsumer(config, watchdog),\n        schema_registry=None,\n        headers_handler=None,\n        value_deserializer=SchemaLessJSONDeserializer(),\n        key_deserializer=StringDeserializer(),\n        stream_result=False,\n    )\n</code></pre>"},{"location":"pages/API/#schemalessjsonstringproducer","title":"SchemaLessJSONStringProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send any value as JSON without any schema.</p> Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>class SchemaLessJSONStringProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send any value as JSON without any schema.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value schema to be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n        \"\"\"\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=None,\n            header_packer=None,\n            value_serializer=SchemaLessJSONSerializer(),\n            key_serializer=StringSerializer(),\n            mapping=mapping,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.schemaless.SchemaLessJSONStringProducer.__init__","title":"<code>__init__(mapping, config)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value schema to be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value schema to be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n    \"\"\"\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=None,\n        header_packer=None,\n        value_serializer=SchemaLessJSONSerializer(),\n        key_serializer=StringSerializer(),\n        mapping=mapping,\n    )\n</code></pre>"},{"location":"pages/API/#schemalessjsonmodelstringproducer","title":"SchemaLessJSONModelStringProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send any instance of pydantic model as JSON without any schema.</p> Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>class SchemaLessJSONModelStringProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send any instance of pydantic model as JSON without any schema.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value schema to be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n        \"\"\"\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=None,\n            header_packer=None,\n            value_serializer=SchemaLessJSONModelSerializer(),\n            key_serializer=StringSerializer(),\n            mapping=mapping,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.schemaless.SchemaLessJSONModelStringProducer.__init__","title":"<code>__init__(mapping, config)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value schema to be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required Source code in <code>wunderkafka/factories/schemaless.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value schema to be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n    \"\"\"\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=None,\n        header_packer=None,\n        value_serializer=SchemaLessJSONModelSerializer(),\n        key_serializer=StringSerializer(),\n        mapping=mapping,\n    )\n</code></pre>"},{"location":"pages/API/#avro","title":"Avro","text":""},{"location":"pages/API/#avroconsumer","title":"AvroConsumer","text":"<p>               Bases: <code>HighLevelDeserializingConsumer</code></p> <p>Kafka Consumer client to get AVRO-serialized messages from Confluent/Cloudera installation.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>class AvroConsumer(HighLevelDeserializingConsumer):\n    \"\"\"Kafka Consumer client to get AVRO-serialized messages from Confluent/Cloudera installation.\"\"\"\n\n    def __init__(\n        self,\n        config: ConsumerConfig,\n        *,\n        sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init consumer from pre-defined blocks.\n\n        :param config:      Configuration for:\n\n                                - Librdkafka consumer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        self._default_timeout: int = 60\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ClouderaSRClient\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            consumer=BytesConsumer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                       cmd_kinit=config.sasl_kerberos_kinit_cmd),\n                SimpleCache(),\n            ),\n            headers_handler=ConfluentClouderaHeadersHandler().parse,\n            deserializer=FastAvroDeserializer(),\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.AvroConsumer.__init__","title":"<code>__init__(config, *, sr_client=None)</code>","text":"<p>Init consumer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Configuration for:  - Librdkafka consumer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>def __init__(\n    self,\n    config: ConsumerConfig,\n    *,\n    sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n) -&gt; None:\n    \"\"\"\n    Init consumer from pre-defined blocks.\n\n    :param config:      Configuration for:\n\n                            - Librdkafka consumer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    self._default_timeout: int = 60\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ClouderaSRClient\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        consumer=BytesConsumer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                   cmd_kinit=config.sasl_kerberos_kinit_cmd),\n            SimpleCache(),\n        ),\n        headers_handler=ConfluentClouderaHeadersHandler().parse,\n        deserializer=FastAvroDeserializer(),\n    )\n</code></pre>"},{"location":"pages/API/#avroproducer","title":"AvroProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send dictionaries or built-in types as messages.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>class AvroProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send dictionaries or built-in types as messages.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n        protocol_id: int = 1\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value schema to be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n        :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ClouderaSRClient\n        self._default_timeout: int = 60\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                       cmd_kinit=config.sasl_kerberos_kinit_cmd),\n                SimpleCache(),\n            ),\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            serializer=FastAvroSerializer(),\n            store=SchemaTextRepo(schema_type=SchemaType.AVRO),\n            mapping=mapping,\n            protocol_id=protocol_id\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.AvroProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None, protocol_id=1)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value schema to be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]]</code> <p>Client for schema registry</p> <code>None</code> <code>protocol_id</code> <code>int</code> <p>Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n    protocol_id: int = 1\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value schema to be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n    :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ClouderaSRClient\n    self._default_timeout: int = 60\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                   cmd_kinit=config.sasl_kerberos_kinit_cmd),\n            SimpleCache(),\n        ),\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        serializer=FastAvroSerializer(),\n        store=SchemaTextRepo(schema_type=SchemaType.AVRO),\n        mapping=mapping,\n        protocol_id=protocol_id\n    )\n</code></pre>"},{"location":"pages/API/#avromodelproducer","title":"AvroModelProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send models or dataclasses as messages.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>class AvroModelProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send models or dataclasses as messages.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n        protocol_id: int = 1\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value model to derive schema which will\n                            be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n        :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n\n        if sr_client is None:\n            sr_client = ClouderaSRClient\n        self._default_timeout: int = 60\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                       cmd_kinit=config.sasl_kerberos_kinit_cmd),\n                SimpleCache(),\n            ),\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            serializer=AvroModelSerializer(),\n            store=AvroModelRepo(),\n            mapping=mapping,\n            protocol_id=protocol_id\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.AvroModelProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None, protocol_id=1)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value model to derive schema which will be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]]</code> <p>Client for schema registry</p> <code>None</code> <code>protocol_id</code> <code>int</code> <p>Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/avro.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n    protocol_id: int = 1\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value model to derive schema which will\n                        be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n    :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n\n    if sr_client is None:\n        sr_client = ClouderaSRClient\n    self._default_timeout: int = 60\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(sr, requires_kerberos=config_requires_kerberos(config),\n                                   cmd_kinit=config.sasl_kerberos_kinit_cmd),\n            SimpleCache(),\n        ),\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        serializer=AvroModelSerializer(),\n        store=AvroModelRepo(),\n        mapping=mapping,\n        protocol_id=protocol_id\n    )\n</code></pre>"},{"location":"pages/API/#json","title":"JSON","text":""},{"location":"pages/API/#jsonconsumer","title":"JSONConsumer","text":"<p>               Bases: <code>HighLevelDeserializingConsumer</code></p> <p>Kafka Consumer client to get JSON-serialized messages from Confluent/Cloudera installation.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>class JSONConsumer(HighLevelDeserializingConsumer):\n    \"\"\"Kafka Consumer client to get JSON-serialized messages from Confluent/Cloudera installation.\"\"\"\n\n    def __init__(\n        self,\n        config: ConsumerConfig,\n        *,\n        sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init consumer from pre-defined blocks.\n\n        :param config:      Configuration for:\n\n                                - Librdkafka consumer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        self._default_timeout: int = 60\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ClouderaSRClient\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            consumer=BytesConsumer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(\n                    sr,\n                    requires_kerberos=config_requires_kerberos(config),\n                    cmd_kinit=config.sasl_kerberos_kinit_cmd,\n                ),\n                SimpleCache(),\n            ),\n            headers_handler=ConfluentClouderaHeadersHandler().parse,\n            deserializer=JSONDeserializer(),\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.json.JSONConsumer.__init__","title":"<code>__init__(config, *, sr_client=None)</code>","text":"<p>Init consumer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Configuration for:  - Librdkafka consumer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>def __init__(\n    self,\n    config: ConsumerConfig,\n    *,\n    sr_client: Optional[Union[Type[ClouderaSRClient], Type[ConfluentSRClient]]] = None,\n) -&gt; None:\n    \"\"\"\n    Init consumer from pre-defined blocks.\n\n    :param config:      Configuration for:\n\n                            - Librdkafka consumer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    self._default_timeout: int = 60\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ClouderaSRClient\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        consumer=BytesConsumer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(\n                sr,\n                requires_kerberos=config_requires_kerberos(config),\n                cmd_kinit=config.sasl_kerberos_kinit_cmd,\n            ),\n            SimpleCache(),\n        ),\n        headers_handler=ConfluentClouderaHeadersHandler().parse,\n        deserializer=JSONDeserializer(),\n    )\n</code></pre>"},{"location":"pages/API/#jsonproducer","title":"JSONProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send dictionaries or built-in types as messages.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>class JSONProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send dictionaries or built-in types as messages.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n        protocol_id: int = 1\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value schema to be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n        :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n        self._default_timeout: int = 60\n\n        config, watchdog = check_watchdog(config)\n        schema_registry = sr_client(\n            KerberizableHTTPClient(\n                sr, requires_kerberos=config_requires_kerberos(config), cmd_kinit=config.sasl_kerberos_kinit_cmd,\n            ),\n            SimpleCache(),\n        )\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=schema_registry,\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            serializer=JSONSerializer(schema_registry.client),\n            store=SchemaTextRepo(schema_type=SchemaType.JSON),\n            mapping=mapping,\n            protocol_id=protocol_id\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.json.JSONProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None, protocol_id=1)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value schema to be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <code>protocol_id</code> <code>int</code> <p>Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n    protocol_id: int = 1\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value schema to be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n    :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n    self._default_timeout: int = 60\n\n    config, watchdog = check_watchdog(config)\n    schema_registry = sr_client(\n        KerberizableHTTPClient(\n            sr, requires_kerberos=config_requires_kerberos(config), cmd_kinit=config.sasl_kerberos_kinit_cmd,\n        ),\n        SimpleCache(),\n    )\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=schema_registry,\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        serializer=JSONSerializer(schema_registry.client),\n        store=SchemaTextRepo(schema_type=SchemaType.JSON),\n        mapping=mapping,\n        protocol_id=protocol_id\n    )\n</code></pre>"},{"location":"pages/API/#jsonmodelproducer","title":"JSONModelProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to serialize and send models or dataclasses as messages.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>class JSONModelProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to serialize and send models or dataclasses as messages.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n        protocol_id: int = 1\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value model to derive schema which will\n                            be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n        :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        sr = config.sr\n        if sr is None:\n            raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n        self._default_timeout: int = 60\n\n        config, watchdog = check_watchdog(config)\n        schema_registry = sr_client(\n            KerberizableHTTPClient(\n                sr,\n                requires_kerberos=config_requires_kerberos(config),\n                cmd_kinit=config.sasl_kerberos_kinit_cmd,\n            ),\n            SimpleCache(),\n        )\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=schema_registry,\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            serializer=JSONModelSerializer(schema_registry.client),\n            store=JSONModelRepo(),\n            mapping=mapping,\n            protocol_id=protocol_id\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.json.JSONModelProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None, protocol_id=1)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value model to derive schema which will be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <code>protocol_id</code> <code>int</code> <p>Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/json.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n    protocol_id: int = 1\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value model to derive schema which will\n                        be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n    :param protocol_id: Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    sr = config.sr\n    if sr is None:\n        raise ValueError('Schema registry config is necessary for {0}'.format(self.__class__.__name__))\n\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n    self._default_timeout: int = 60\n\n    config, watchdog = check_watchdog(config)\n    schema_registry = sr_client(\n        KerberizableHTTPClient(\n            sr,\n            requires_kerberos=config_requires_kerberos(config),\n            cmd_kinit=config.sasl_kerberos_kinit_cmd,\n        ),\n        SimpleCache(),\n    )\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=schema_registry,\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        serializer=JSONModelSerializer(schema_registry.client),\n        store=JSONModelRepo(),\n        mapping=mapping,\n        protocol_id=protocol_id\n    )\n</code></pre>"},{"location":"pages/API/#mixed","title":"Mixed","text":"<p>These pre-configured consumers and producers are provided for convenience and as explicit example of how to define own factories with mixed (de)serializers.</p> <p>Note</p> <p>The naming follow producer API, where the key follows the value. That\\'s why the classes are called: Value(de)Serializer + Key(de)Serializer.</p>"},{"location":"pages/API/#avro-string","title":"AVRO + String","text":""},{"location":"pages/API/#avromodelstringproducer","title":"AvroModelStringProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to send models as avro-serialized message values and string-serialized keys.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>class AvroModelStringProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to send models as avro-serialized message values and string-serialized keys.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value model to derive schema which will\n                            be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        self._default_timeout: int = 60\n        sr = config.sr\n        if sr is None:\n            raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(\n                    sr,\n                    requires_kerberos=config_requires_kerberos(config),\n                    cmd_kinit=config.sasl_kerberos_kinit_cmd,\n                ),\n                SimpleCache()),\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            value_serializer=AvroModelSerializer(AvroModelRepo()),\n            key_serializer=StringSerializer(),\n            mapping=mapping,\n            protocol_id=0,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.mixed.AvroModelStringProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value model to derive schema which will be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value model to derive schema which will\n                        be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    self._default_timeout: int = 60\n    sr = config.sr\n    if sr is None:\n        raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(\n                sr,\n                requires_kerberos=config_requires_kerberos(config),\n                cmd_kinit=config.sasl_kerberos_kinit_cmd,\n            ),\n            SimpleCache()),\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        value_serializer=AvroModelSerializer(AvroModelRepo()),\n        key_serializer=StringSerializer(),\n        mapping=mapping,\n        protocol_id=0,\n    )\n</code></pre>"},{"location":"pages/API/#avrostringconsumer","title":"AvroStringConsumer","text":"<p>               Bases: <code>HighLevelDeserializingConsumer</code></p> <p>Kafka Consumer client to get messages with avro-serialized values and string-serialized keys.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>class AvroStringConsumer(HighLevelDeserializingConsumer):\n    \"\"\"Kafka Consumer client to get messages with avro-serialized values and string-serialized keys.\"\"\"\n\n    def __init__(\n        self,\n        config: ConsumerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init consumer from pre-defined blocks.\n\n        :param config:      Configuration for:\n\n                                - Librdkafka consumer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        self._default_timeout: int = 60\n        sr = config.sr\n        if sr is None:\n            raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            consumer=BytesConsumer(config, None),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(sr),\n                SimpleCache(),\n            ),\n            headers_handler=ConfluentClouderaHeadersHandler().parse,\n            value_deserializer=FastAvroDeserializer(),\n            key_deserializer=StringDeserializer(),\n            stream_result=True,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.mixed.AvroStringConsumer.__init__","title":"<code>__init__(config, *, sr_client=None)</code>","text":"<p>Init consumer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Configuration for:  - Librdkafka consumer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>def __init__(\n    self,\n    config: ConsumerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n) -&gt; None:\n    \"\"\"\n    Init consumer from pre-defined blocks.\n\n    :param config:      Configuration for:\n\n                            - Librdkafka consumer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    self._default_timeout: int = 60\n    sr = config.sr\n    if sr is None:\n        raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        consumer=BytesConsumer(config, None),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(sr),\n            SimpleCache(),\n        ),\n        headers_handler=ConfluentClouderaHeadersHandler().parse,\n        value_deserializer=FastAvroDeserializer(),\n        key_deserializer=StringDeserializer(),\n        stream_result=True,\n    )\n</code></pre>"},{"location":"pages/API/#json-string","title":"JSON + String","text":""},{"location":"pages/API/#jsonmodelstringproducer","title":"JSONModelStringProducer","text":"<p>               Bases: <code>HighLevelSerializingProducer</code></p> <p>Kafka Producer client to send models as JSON-serialized message values and string-serialized keys.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>class JSONModelStringProducer(HighLevelSerializingProducer):\n    \"\"\"Kafka Producer client to send models as JSON-serialized message values and string-serialized keys.\"\"\"\n\n    def __init__(\n        self,\n        mapping: Optional[Dict[TopicName, MessageDescription]],\n        config: ProducerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init producer from pre-defined blocks.\n\n        :param mapping:     Topic-to-Schemas mapping.\n                            Mapping's value should contain at least message's value model to derive schema which will\n                            be used for serialization.\n        :param config:      Configuration for:\n\n                                - Librdkafka producer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        self._default_timeout: int = 60\n\n        sr = config.sr\n        if sr is None:\n            raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n\n        config, watchdog = check_watchdog(config)\n        schema_registry = sr_client(\n            KerberizableHTTPClient(\n                sr,\n                requires_kerberos=config_requires_kerberos(config),\n                cmd_kinit=config.sasl_kerberos_kinit_cmd,\n            ),\n            SimpleCache(),\n        )\n        super().__init__(\n            producer=BytesProducer(config, watchdog),\n            schema_registry=schema_registry,\n            header_packer=ConfluentClouderaHeadersHandler().pack,\n            value_serializer=JSONModelSerializer(schema_registry.client),\n            key_serializer=StringSerializer(),\n            mapping=mapping,\n            protocol_id=0,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.mixed.JSONModelStringProducer.__init__","title":"<code>__init__(mapping, config, *, sr_client=None)</code>","text":"<p>Init producer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Topic-to-Schemas mapping. Mapping's value should contain at least message's value model to derive schema which will be used for serialization.</p> required <code>config</code> <code>ProducerConfig</code> <p>Configuration for:  - Librdkafka producer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>def __init__(\n    self,\n    mapping: Optional[Dict[TopicName, MessageDescription]],\n    config: ProducerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n) -&gt; None:\n    \"\"\"\n    Init producer from pre-defined blocks.\n\n    :param mapping:     Topic-to-Schemas mapping.\n                        Mapping's value should contain at least message's value model to derive schema which will\n                        be used for serialization.\n    :param config:      Configuration for:\n\n                            - Librdkafka producer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    self._default_timeout: int = 60\n\n    sr = config.sr\n    if sr is None:\n        raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n\n    config, watchdog = check_watchdog(config)\n    schema_registry = sr_client(\n        KerberizableHTTPClient(\n            sr,\n            requires_kerberos=config_requires_kerberos(config),\n            cmd_kinit=config.sasl_kerberos_kinit_cmd,\n        ),\n        SimpleCache(),\n    )\n    super().__init__(\n        producer=BytesProducer(config, watchdog),\n        schema_registry=schema_registry,\n        header_packer=ConfluentClouderaHeadersHandler().pack,\n        value_serializer=JSONModelSerializer(schema_registry.client),\n        key_serializer=StringSerializer(),\n        mapping=mapping,\n        protocol_id=0,\n    )\n</code></pre>"},{"location":"pages/API/#jsonstringconsumer","title":"JSONStringConsumer","text":"<p>               Bases: <code>HighLevelDeserializingConsumer</code></p> <p>Kafka Consumer client to get messages with JSON-serialized values and string-serialized keys.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>class JSONStringConsumer(HighLevelDeserializingConsumer):\n    \"\"\"Kafka Consumer client to get messages with JSON-serialized values and string-serialized keys.\"\"\"\n\n    def __init__(\n        self,\n        config: ConsumerConfig,\n        *,\n        sr_client: Optional[Type[ConfluentSRClient]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init consumer from pre-defined blocks.\n\n        :param config:      Configuration for:\n\n                                - Librdkafka consumer.\n                                - Schema registry client (conventional options for HTTP).\n\n                            Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n        :param sr_client:   Client for schema registry\n\n        :raises ValueError: If schema registry configuration is missing.\n        \"\"\"\n        self._default_timeout: int = 60\n        sr = config.sr\n        if sr is None:\n            raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n        if sr_client is None:\n            sr_client = ConfluentSRClient\n\n        config, watchdog = check_watchdog(config)\n        super().__init__(\n            consumer=BytesConsumer(config, watchdog),\n            schema_registry=sr_client(\n                KerberizableHTTPClient(sr),\n                SimpleCache(),\n            ),\n            headers_handler=ConfluentClouderaHeadersHandler().parse,\n            value_deserializer=JSONDeserializer(),\n            key_deserializer=StringDeserializer(),\n            stream_result=True,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.factories.mixed.JSONStringConsumer.__init__","title":"<code>__init__(config, *, sr_client=None)</code>","text":"<p>Init consumer from pre-defined blocks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConsumerConfig</code> <p>Configuration for:  - Librdkafka consumer. - Schema registry client (conventional options for HTTP).  Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.</p> required <code>sr_client</code> <code>Optional[Type[ConfluentSRClient]]</code> <p>Client for schema registry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If schema registry configuration is missing.</p> Source code in <code>wunderkafka/factories/mixed.py</code> <pre><code>def __init__(\n    self,\n    config: ConsumerConfig,\n    *,\n    sr_client: Optional[Type[ConfluentSRClient]] = None,\n) -&gt; None:\n    \"\"\"\n    Init consumer from pre-defined blocks.\n\n    :param config:      Configuration for:\n\n                            - Librdkafka consumer.\n                            - Schema registry client (conventional options for HTTP).\n\n                        Refer original CONFIGURATION.md (https://git.io/JmgCl) or generated config.\n\n    :param sr_client:   Client for schema registry\n\n    :raises ValueError: If schema registry configuration is missing.\n    \"\"\"\n    self._default_timeout: int = 60\n    sr = config.sr\n    if sr is None:\n        raise ValueError(\"Schema registry config is necessary for {0}\".format(self.__class__.__name__))\n    if sr_client is None:\n        sr_client = ConfluentSRClient\n\n    config, watchdog = check_watchdog(config)\n    super().__init__(\n        consumer=BytesConsumer(config, watchdog),\n        schema_registry=sr_client(\n            KerberizableHTTPClient(sr),\n            SimpleCache(),\n        ),\n        headers_handler=ConfluentClouderaHeadersHandler().parse,\n        value_deserializer=JSONDeserializer(),\n        key_deserializer=StringDeserializer(),\n        stream_result=True,\n    )\n</code></pre>"},{"location":"pages/API/#topicsubscription","title":"TopicSubscription","text":"<p>               Bases: <code>object</code></p> <p>Allows custom definition of subscription for topic without need to build full TopicPartition list.</p> Source code in <code>wunderkafka/consumers/subscription.py</code> <pre><code>class TopicSubscription(object):\n    \"\"\"Allows custom definition of subscription for topic without need to build full TopicPartition list.\"\"\"\n\n    def __init__(  # noqa: WPS211  # ToDo (tribunsky.kir): reconsider API of 'how'\n        self,\n        topic: str,\n        *,\n        from_beginning: Optional[bool] = None,\n        offset: Optional[int] = None,\n        ts: Optional[int] = None,\n        with_timedelta: Optional[datetime.timedelta] = None,\n    ) -&gt; None:\n        \"\"\"\n        Init topic subscription object.\n\n        Only one method of subscription per topic is allowed at time:\n\n        - from beginning (depends on your retention policy)\n        - from end (consume only latest messages)\n        - via specific offset\n        - via specific timestamp\n        - via specific timedelta (from current datetime)\n        - no special option (consumer will use \"default\" value of auto.offset.reset)\n\n        :param topic:           Topic to subscribe.\n        :param from_beginning:  If True, subscribe to get earliest available messages. If False, get latest messages.\n        :param offset:          Subscribe to specific offset.\n                                If offset not found, will behave with built-in default.\n        :param ts:              Subscribe to specific timestamp (milliseconds).\n                                If timestamp not found, will behave with built-in default.\n        :param with_timedelta:  Subscribe to some moment in the past, from current datetime for a given timedelta.\n                                Will calculate specific timestamp and subscribe via ts.\n        \"\"\"\n        self.topic = topic\n        self.how = choose_offset(\n            from_beginning=from_beginning, offset=offset, ts=ts, with_timedelta=with_timedelta,\n        )\n</code></pre>"},{"location":"pages/API/#wunderkafka.TopicSubscription.__init__","title":"<code>__init__(topic, *, from_beginning=None, offset=None, ts=None, with_timedelta=None)</code>","text":"<p>Init topic subscription object.</p> <p>Only one method of subscription per topic is allowed at time:</p> <ul> <li>from beginning (depends on your retention policy)</li> <li>from end (consume only latest messages)</li> <li>via specific offset</li> <li>via specific timestamp</li> <li>via specific timedelta (from current datetime)</li> <li>no special option (consumer will use \"default\" value of auto.offset.reset)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Topic to subscribe.</p> required <code>from_beginning</code> <code>Optional[bool]</code> <p>If True, subscribe to get earliest available messages. If False, get latest messages.</p> <code>None</code> <code>offset</code> <code>Optional[int]</code> <p>Subscribe to specific offset. If offset not found, will behave with built-in default.</p> <code>None</code> <code>ts</code> <code>Optional[int]</code> <p>Subscribe to specific timestamp (milliseconds). If timestamp not found, will behave with built-in default.</p> <code>None</code> <code>with_timedelta</code> <code>Optional[timedelta]</code> <p>Subscribe to some moment in the past, from current datetime for a given timedelta. Will calculate specific timestamp and subscribe via ts.</p> <code>None</code> Source code in <code>wunderkafka/consumers/subscription.py</code> <pre><code>def __init__(  # noqa: WPS211  # ToDo (tribunsky.kir): reconsider API of 'how'\n    self,\n    topic: str,\n    *,\n    from_beginning: Optional[bool] = None,\n    offset: Optional[int] = None,\n    ts: Optional[int] = None,\n    with_timedelta: Optional[datetime.timedelta] = None,\n) -&gt; None:\n    \"\"\"\n    Init topic subscription object.\n\n    Only one method of subscription per topic is allowed at time:\n\n    - from beginning (depends on your retention policy)\n    - from end (consume only latest messages)\n    - via specific offset\n    - via specific timestamp\n    - via specific timedelta (from current datetime)\n    - no special option (consumer will use \"default\" value of auto.offset.reset)\n\n    :param topic:           Topic to subscribe.\n    :param from_beginning:  If True, subscribe to get earliest available messages. If False, get latest messages.\n    :param offset:          Subscribe to specific offset.\n                            If offset not found, will behave with built-in default.\n    :param ts:              Subscribe to specific timestamp (milliseconds).\n                            If timestamp not found, will behave with built-in default.\n    :param with_timedelta:  Subscribe to some moment in the past, from current datetime for a given timedelta.\n                            Will calculate specific timestamp and subscribe via ts.\n    \"\"\"\n    self.topic = topic\n    self.how = choose_offset(\n        from_beginning=from_beginning, offset=offset, ts=ts, with_timedelta=with_timedelta,\n    )\n</code></pre>"},{"location":"pages/advanced_API/","title":"Advanced API","text":""},{"location":"pages/advanced_API/#api-for-extension","title":"API for extension","text":"<p>Docs are under construction, but code is ready.</p>"},{"location":"pages/advanced_API/#high-level-pipelines","title":"High-level pipelines","text":""},{"location":"pages/advanced_API/#consumer","title":"Consumer","text":"<p>               Bases: <code>AbstractDeserializingConsumer</code></p> <p>Deserializing pipeline implementation of extended consumer.</p> Source code in <code>wunderkafka/consumers/constructor.py</code> <pre><code>class HighLevelDeserializingConsumer(AbstractDeserializingConsumer):\n    \"\"\"Deserializing pipeline implementation of extended consumer.\"\"\"\n\n    def __init__(\n        self,\n        consumer: AbstractConsumer,\n        headers_handler: Optional[HeaderParser],\n        schema_registry: Optional[AbstractSchemaRegistry],\n        deserializer: Optional[AbstractDeserializer] = None,\n        value_deserializer: Optional[AbstractDeserializer] = None,\n        key_deserializer: Optional[AbstractDeserializer] = None,\n        *,\n        stream_result: bool = False,\n    ):\n        \"\"\"\n        Init consumer with specific dependencies.\n\n        :param consumer:            Consumer implementation to receive messages.\n        :param headers_handler:     Callable to parse binary headers.\n        :param schema_registry:     Schema registry client.\n        :param deserializer:        Common message deserializer for value and key, if set.\n        :param value_deserializer:  Message deserializer for value, if set.\n        :param key_deserializer:    Message deserializer for the key, if set.\n        :param stream_result:       If True, return a complex StreamResult object instead of just a model.\n        \"\"\"\n        self.consumer = consumer\n        self._header_parser = headers_handler\n        self._registry = schema_registry\n        self._deserializer = deserializer\n\n        self._value_deserializer = choose_one_of(deserializer, value_deserializer, 'value')\n        self._key_deserializer = choose_one_of(deserializer, key_deserializer, 'key')\n\n        self._stream_result = stream_result\n\n    def subscribe(  # noqa: D102,WPS211 # docstring inherited from superclass.\n        self,\n        topics: List[Union[str, TopicSubscription]],\n        *,\n        from_beginning: Optional[bool] = None,\n        offset: Optional[int] = None,\n        ts: Optional[int] = None,\n        with_timedelta: Optional[datetime.timedelta] = None,\n    ) -&gt; None:\n        self.consumer.subscribe(\n            topics, from_beginning=from_beginning, offset=offset, ts=ts, with_timedelta=with_timedelta,\n        )\n\n    def commit(  # noqa: D102,WPS211 # docstring inherited from superclass.\n        self,\n        message: Optional[Message] = None,\n        offsets: Optional[List[TopicPartition]] = None,\n        asynchronous: bool = True,\n    ) -&gt; Optional[List[TopicPartition]]:\n        if message is None and offsets is not None:\n            return self.consumer.commit(offsets=offsets, asynchronous=asynchronous)\n        if message is not None and offsets is None:\n            return self.consumer.commit(message=message, asynchronous=asynchronous)\n        # Default behavior\n        return self.consumer.commit(message=message, offsets=offsets, asynchronous=asynchronous)\n\n    def consume(\n        self,\n        timeout: float = 1.0,\n        num_messages: int = 1000000,\n        *,\n        ignore_keys: bool = False,\n        raise_on_error: bool = True,\n        raise_on_lost: bool = True,\n    ) -&gt; List[T]:\n        \"\"\"\n        Consume as many messages as we can for a given timeout and decode them.\n\n        :param timeout:         The maximum time to block waiting for messages. Decoding time doesn't count.\n        :param num_messages:    The maximum number of messages to receive from broker.\n                                Default is 1000000 that was the allowed maximum for librdkafka 1.2.\n        :param ignore_keys:     If True, skip key decoding, key will be set to None. Otherwise, decode key as usual.\n        :param raise_on_error:  If True, raise KafkaError form confluent_kafka library to handle in client code.\n        :param raise_on_lost:   If True, check on own clocks if max.poll.interval.ms is exceeded. If so, raises\n                                ConsumerException to be handled in client code.\n\n        :raises KafkaError:     See https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent_kafka.KafkaError  # noqa: E501\n\n        :return:                A list of Message objects with decoded value() and key() (possibly empty on timeout).\n        \"\"\"  # noqa: E501\n        msgs = self.consumer.batch_poll(timeout, num_messages, raise_on_lost=raise_on_lost)\n        return self._decoded(\n            msgs,\n            ignore_keys=ignore_keys,\n            raise_on_error=raise_on_error,\n        )\n\n    def _decoded(self, msgs: List[Message], *, ignore_keys: bool, raise_on_error: bool) -&gt; List[T]:\n        results: List[StreamResult] = []\n        for msg in msgs:\n            kafka_error = msg.error()\n            # Not sure if there is any need for an option to exclude errored message from the consumed ones,\n            # so there is no `else`. In case when `.error()` returns KafkaError, but `raise_on_error` is set to False,\n            # `.value()` called in client code will return raw bytes with string from KafkaError,\n            # e.g.\n            #   b'Application maximum poll interval (300000ms) exceeded by Nms'\n            if kafka_error is not None:\n                logger.error(kafka_error)\n                if raise_on_error:\n                    # Even PyCharm stubs show that it is inherited from an object, in fact it is a valid Exception\n                    raise kafka_error\n\n            topic = msg.topic()\n\n            raw_key_value = msg.key()\n            decode_key_ok = True\n            if ignore_keys:\n                # Yes, we lose information, but it is necessary to not get raw bytes\n                # if `.key()` will be called in client code later.\n                msg.set_key(None)\n            else:\n                try:\n                    decoded_key = self._decode(topic, msg.key(), is_key=True)\n                # KeyDeserializationError is inherited from SerializationError\n                except SerializationError:\n                    decode_key_ok = False\n                    logger.error(\"Unable to decode key from bytes: {0}\".format(raw_key_value))\n                    if not ignore_keys:\n                        raise\n                else:\n                    msg.set_key(decoded_key)\n\n            try:\n                decoded_value = self._decode(topic, msg.value())\n            except (SerializationError, ValueError) as exc:\n                logger.error(\"Unable to decode value from bytes: {0}\".format(msg.value()))\n                if not self._stream_result:\n                    raise\n                value_error = str(exc)\n                if decode_key_ok:\n                    error = PayloadError(description=value_error)\n                else:\n                    message = \"Unable to decode key (topic: {0}, key payload: {1})\".format(topic, raw_key_value)\n                    error = PayloadError(description=message)\n                results.append(StreamResult(payload=None, error=error, msg=msg))\n            else:\n                msg.set_value(decoded_value)\n                if self._stream_result:\n                    results.append(StreamResult(payload=decoded_value, error=None, msg=msg))\n\n        to_return = results if self._stream_result else msgs\n        return to_return\n\n    # Todo (tribunsky.kir): arguable: make different composition (headers, SR &amp; deserializer united cache)\n    def _decode(self, topic: str, blob: Optional[bytes], *, is_key: bool = False) -&gt; Any:\n        if blob is None:\n            return None\n\n        deserializer = self._get_deserializer(is_key)\n        if deserializer.schemaless:\n            return deserializer.deserialize('', blob)\n        # Header is separate in the sake of customization, e.g., we don't have SR and put schema directly in a message\n        assert self._header_parser is not None\n        assert self._registry is not None\n        parsed_header = self._header_parser(blob)\n        schema_meta = SchemaMeta(\n            topic=topic,\n            is_key=is_key,\n            header=parsed_header,\n        )\n        schema_text = self._registry.get_schema_text(schema_meta)\n        schema = SchemaDescription(text=schema_text)\n        # performance tradeoff: a message may be long, and we don't want to:\n        # - copy the whole tail\n        # - have implicit offset as if we read buffer when extracting header\n        # so dealing with implicit offset and the whole binary string\n        return deserializer.deserialize(schema.text, blob, seek_pos=parsed_header.size)\n\n    def _get_deserializer(self, is_key: bool) -&gt; AbstractDeserializer:\n        return self._key_deserializer if is_key else self._value_deserializer\n</code></pre>"},{"location":"pages/advanced_API/#wunderkafka.consumers.constructor.HighLevelDeserializingConsumer.__init__","title":"<code>__init__(consumer, headers_handler, schema_registry, deserializer=None, value_deserializer=None, key_deserializer=None, *, stream_result=False)</code>","text":"<p>Init consumer with specific dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>consumer</code> <code>AbstractConsumer</code> <p>Consumer implementation to receive messages.</p> required <code>headers_handler</code> <code>Optional[HeaderParser]</code> <p>Callable to parse binary headers.</p> required <code>schema_registry</code> <code>Optional[AbstractSchemaRegistry]</code> <p>Schema registry client.</p> required <code>deserializer</code> <code>Optional[AbstractDeserializer]</code> <p>Common message deserializer for value and key, if set.</p> <code>None</code> <code>value_deserializer</code> <code>Optional[AbstractDeserializer]</code> <p>Message deserializer for value, if set.</p> <code>None</code> <code>key_deserializer</code> <code>Optional[AbstractDeserializer]</code> <p>Message deserializer for the key, if set.</p> <code>None</code> <code>stream_result</code> <code>bool</code> <p>If True, return a complex StreamResult object instead of just a model.</p> <code>False</code> Source code in <code>wunderkafka/consumers/constructor.py</code> <pre><code>def __init__(\n    self,\n    consumer: AbstractConsumer,\n    headers_handler: Optional[HeaderParser],\n    schema_registry: Optional[AbstractSchemaRegistry],\n    deserializer: Optional[AbstractDeserializer] = None,\n    value_deserializer: Optional[AbstractDeserializer] = None,\n    key_deserializer: Optional[AbstractDeserializer] = None,\n    *,\n    stream_result: bool = False,\n):\n    \"\"\"\n    Init consumer with specific dependencies.\n\n    :param consumer:            Consumer implementation to receive messages.\n    :param headers_handler:     Callable to parse binary headers.\n    :param schema_registry:     Schema registry client.\n    :param deserializer:        Common message deserializer for value and key, if set.\n    :param value_deserializer:  Message deserializer for value, if set.\n    :param key_deserializer:    Message deserializer for the key, if set.\n    :param stream_result:       If True, return a complex StreamResult object instead of just a model.\n    \"\"\"\n    self.consumer = consumer\n    self._header_parser = headers_handler\n    self._registry = schema_registry\n    self._deserializer = deserializer\n\n    self._value_deserializer = choose_one_of(deserializer, value_deserializer, 'value')\n    self._key_deserializer = choose_one_of(deserializer, key_deserializer, 'key')\n\n    self._stream_result = stream_result\n</code></pre>"},{"location":"pages/advanced_API/#wunderkafka.consumers.constructor.HighLevelDeserializingConsumer.consume","title":"<code>consume(timeout=1.0, num_messages=1000000, *, ignore_keys=False, raise_on_error=True, raise_on_lost=True)</code>","text":"<p>Consume as many messages as we can for a given timeout and decode them.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>The maximum time to block waiting for messages. Decoding time doesn't count.</p> <code>1.0</code> <code>num_messages</code> <code>int</code> <p>The maximum number of messages to receive from broker. Default is 1000000 that was the allowed maximum for librdkafka 1.2.</p> <code>1000000</code> <code>ignore_keys</code> <code>bool</code> <p>If True, skip key decoding, key will be set to None. Otherwise, decode key as usual.</p> <code>False</code> <code>raise_on_error</code> <code>bool</code> <p>If True, raise KafkaError form confluent_kafka library to handle in client code.</p> <code>True</code> <code>raise_on_lost</code> <code>bool</code> <p>If True, check on own clocks if max.poll.interval.ms is exceeded. If so, raises ConsumerException to be handled in client code.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[T]</code> <p>A list of Message objects with decoded value() and key() (possibly empty on timeout).</p> <p>Raises:</p> Type Description <code>KafkaError</code> <p>See https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent_kafka.KafkaError  # noqa: E501</p> Source code in <code>wunderkafka/consumers/constructor.py</code> <pre><code>def consume(\n    self,\n    timeout: float = 1.0,\n    num_messages: int = 1000000,\n    *,\n    ignore_keys: bool = False,\n    raise_on_error: bool = True,\n    raise_on_lost: bool = True,\n) -&gt; List[T]:\n    \"\"\"\n    Consume as many messages as we can for a given timeout and decode them.\n\n    :param timeout:         The maximum time to block waiting for messages. Decoding time doesn't count.\n    :param num_messages:    The maximum number of messages to receive from broker.\n                            Default is 1000000 that was the allowed maximum for librdkafka 1.2.\n    :param ignore_keys:     If True, skip key decoding, key will be set to None. Otherwise, decode key as usual.\n    :param raise_on_error:  If True, raise KafkaError form confluent_kafka library to handle in client code.\n    :param raise_on_lost:   If True, check on own clocks if max.poll.interval.ms is exceeded. If so, raises\n                            ConsumerException to be handled in client code.\n\n    :raises KafkaError:     See https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent_kafka.KafkaError  # noqa: E501\n\n    :return:                A list of Message objects with decoded value() and key() (possibly empty on timeout).\n    \"\"\"  # noqa: E501\n    msgs = self.consumer.batch_poll(timeout, num_messages, raise_on_lost=raise_on_lost)\n    return self._decoded(\n        msgs,\n        ignore_keys=ignore_keys,\n        raise_on_error=raise_on_error,\n    )\n</code></pre>"},{"location":"pages/advanced_API/#producer","title":"Producer","text":"<p>This module contains the high-level pipeline to produce messages with a nested producer.</p> <p>It is intended to be testable enough due to the composition of dependencies.</p> <p>All moving parts should be interchangeable in terms of schema, header and serialization handling (for further overriding^W extending).</p>"},{"location":"pages/advanced_API/#wunderkafka.producers.constructor.HighLevelSerializingProducer","title":"<code>HighLevelSerializingProducer</code>","text":"<p>               Bases: <code>AbstractSerializingProducer</code></p> <p>Serializing pipeline implementation of extended producer.</p> Source code in <code>wunderkafka/producers/constructor.py</code> <pre><code>class HighLevelSerializingProducer(AbstractSerializingProducer):\n    \"\"\"Serializing pipeline implementation of extended producer.\"\"\"\n\n    def __init__(  # noqa: WPS211  # ToDo (tribunsky.kir): rethink building of producer.\n        #                                                  Maybe single Callable (like in kafka-python) much better.\n        self,\n        producer: AbstractProducer,\n        # Some serializers doesn't need SR at all, e.g. StringSerializer.\n        schema_registry: Optional[AbstractSchemaRegistry],\n        # As some serializers doesn't contain magic byte, we do not need to handle the first bytes of a message at all.\n        header_packer: Optional[HeaderPacker],\n        serializer: Optional[AbstractSerializer] = None,\n        store: Optional[AbstractDescriptionStore] = None,\n        # ToDo: switch mapping to something like consumer's TopicSubscription?\n        mapping: Optional[Dict[TopicName, MessageDescription]] = None,\n        value_serializer: Optional[AbstractSerializer] = None,\n        key_serializer: Optional[AbstractSerializer] = None,\n        *,\n        protocol_id: int = 1,\n        lazy: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Init producer with specific dependencies and prepare it to work against specified topic(s).\n\n        :param producer:            Producer implementation to send messages.\n        :param schema_registry:     Schema registry client.\n        :param header_packer:       Callable to form binary headers.\n        :param serializer:          Common message serializer for the key and value.\n                                    If specific value_deserializer/key_deserializer defined, it will be used instead.\n        :param store:               Specific store to provide schema text extraction from schema description.\n        :param mapping:             Per-topic definition of value and/or key schema descriptions.\n        :param value_serializer:    Message serializer for value, if set.\n        :param key_serializer:      Message serializer for the key, if set.\n        :param protocol_id:         Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n        :param lazy:                If True,\n                                    defer schema registry publication, otherwise schema will be registered\n                                    before the first message sending.\n        \"\"\"\n        self._mapping = mapping or {}\n\n        # ToDo (tribunsky.kir): look like wrong place. Maybe it's better to highlight an entity\n        #                       of Schema which may be checked or not. Then input mapping may be eliminated (or not).\n        self._checked: Dict[Tuple[str, str, bool], SRMeta] = {}\n\n        self._store = store\n        self._sr = schema_registry\n        self._serializer = serializer\n        self._producer = producer\n        self._header_packer = header_packer\n        self._protocol_id = protocol_id\n\n        chosen_value_serializer = value_serializer if value_serializer else serializer\n        if chosen_value_serializer is None:\n            msg = 'Value serializer is not specified, should be passed via value_serializer or serializer at least.'\n            raise ValueError(msg)\n        self._value_serializer = chosen_value_serializer\n\n        chosen_key_serializer = key_serializer if value_serializer else serializer\n        if chosen_key_serializer is None:\n            msg = 'Key serializer is not specified, should be passed via key_serializer or serializer at least.'\n            raise ValueError(msg)\n        self._key_serializer = chosen_key_serializer\n\n        for topic, description in self._mapping.items():\n            if isinstance(description, (tuple, list)):\n                msg_value, msg_key = description\n                self.set_target_topic(topic, msg_value, msg_key, lazy=lazy)\n            else:\n                self.set_target_topic(topic, description, lazy=lazy)\n\n    def flush(self, timeout: Optional[float] = None) -&gt; int:  # noqa: D102 # docstring inherited from superclass.\n        if timeout is None:\n            return self._producer.flush()\n        return self._producer.flush(timeout)\n\n    def set_target_topic(  # noqa: D102 # docstring inherited from superclass.\n        self,\n        topic: str,\n        value: Any,  # noqa: WPS110 # Domain. inherited from superclass.\n        key: Any = None,\n        *,\n        lazy: bool = False,\n    ) -&gt; None:\n        value_store = self._get_store(self._value_serializer)\n        key_store = self._get_store(self._key_serializer)\n        value_store.add(topic, value, None)\n        if key is not None:\n            # FixMe (tribunsky.kir): make key and value stores independent,\n            #                        because now we cannot put None instead of value,\n            #                        even though we need store only for the key.\n            key_store.add(topic, value, key)\n        if not lazy:\n            value_descr = value_store.get(topic)\n            self._check_schema(topic, value_descr)\n            if key is not None:\n                key_descr = key_store.get(topic, is_key=True)\n                assert key_descr is not None\n                if not key_descr.empty:\n                    self._check_schema(topic, key_descr, is_key=True)\n\n    def send_message(  # noqa: D102,WPS211 # inherited from superclass.\n        self,\n        topic: str,\n        value: MsgValue = None,  # noqa: WPS110 # Domain. inherited from superclass.\n        key: MsgKey = None,\n        partition: Optional[int] = None,\n        on_delivery: Optional[DeliveryCallback] = error_callback,\n        *args: Any,\n        blocking: bool = False,\n        protocol_id: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        if protocol_id is None:\n            protocol_id = self._protocol_id\n        encoded_value = self._encode(topic, value, protocol_id)\n        encoded_key = self._encode(topic, key, protocol_id, is_key=True)\n\n        self._producer.send_message(\n            topic,\n            encoded_value,\n            encoded_key,\n            partition,\n            on_delivery,\n            blocking=blocking,\n            *args,\n            **kwargs,\n        )\n\n    def _check_schema(\n        self,\n        topic: str,\n        schema: Optional[SchemaDescription],\n        *,\n        is_key: bool = False,\n        force: bool = False,\n    ) -&gt; Optional[SRMeta]:\n        \"\"\"\n        Ensure that we have schema's ids necessary to create message's header.\n\n        :param topic:   Target topic against which we are working for this call.\n        :param schema:  Schema container to be registered. Should contain text.\n        :param is_key:  If True, schema will be requested as a message key, message value otherwise.\n        :param force:   If True, do not reuse cached results, always request Schema Registry.\n\n        :raises ValueError: If received no schema from DescriptionStore.\n\n        :return:        Container with schema's ids.\n        \"\"\"\n        if self._sr is None:\n            logger.warning('Schema registry is not passed, skipping schema check for {0}'.format(topic))\n            return None\n        if schema is None:\n            raise ValueError(\"Couldn't check schema from store.\")\n        uid = (topic, schema.text, is_key)\n\n        if force is False:\n            meta = self._checked.get(uid)\n            if meta is not None:\n                return meta\n\n        assert schema.type is not None\n        meta = self._sr.register_schema(topic, schema.text, schema.type, is_key=is_key)\n        self._checked[uid] = meta\n        return meta\n\n    # ToDo (tribunsky.kir): Maybe, this method should be public for more simple extension.\n    #                       'Template' pattern never works anyway.\n    def _encode(self, topic: str, obj: Any, protocol_id: int, is_key: bool = False) -&gt; Optional[bytes]:  # noqa: WPS110\n        if obj is None:\n            return None\n\n        serializer = self._get_serializer(is_key)\n        store = self._get_store(serializer)\n\n        schema = store.get(topic, is_key=is_key)\n        if schema is None:\n            logger.warning('Missing schema for {0} (key: {1}'.format(topic, is_key))\n            return None\n        if schema.empty:\n            return serializer.serialize(schema.text, obj, None, topic, is_key=is_key)\n        else:\n            available_meta = self._check_schema(topic, schema, is_key=is_key)\n            # ToDo (tribunsky.kir): `_check_schema()` for now return Optional cause it is used when setting\n            #                       producer per topic and should not push schema to on schemaless serializers.\n            assert available_meta is not None\n            # ToDo (tribunsky.kir): looks like header handler should be also placed per-payload or per-topic,\n            #                       because some serializers doesn't use it (e.g. confluent string serializer)\n            assert self._header_packer is not None\n            # ToDo (tribunsky.kir): check if old client uses\n            #                       '{\"schema\": \"{\\\"type\\\": \\\"string\\\"}\"}'\n            #                       https://docs.confluent.io/platform/current/schema-registry/develop/using.html#common-sr-api-usage-examples  # noqa: E501\n            #                       Looks like the new one doesn't publish string schema at all\n            #                       (no type in library for that)\n            header = self._header_packer(protocol_id, available_meta)\n            return serializer.serialize(schema.text, obj, header, topic, is_key=is_key)\n\n    def _get_store(self, serializer: AbstractSerializer) -&gt; AbstractDescriptionStore:\n        serializers_store = getattr(serializer, 'store', None)\n        if serializers_store is not None:\n            return serializers_store\n        assert self._store is not None\n        return self._store\n\n    def _get_serializer(self, is_key: bool) -&gt; AbstractSerializer:\n        return self._key_serializer if is_key else self._value_serializer\n</code></pre>"},{"location":"pages/advanced_API/#wunderkafka.producers.constructor.HighLevelSerializingProducer.__init__","title":"<code>__init__(producer, schema_registry, header_packer, serializer=None, store=None, mapping=None, value_serializer=None, key_serializer=None, *, protocol_id=1, lazy=False)</code>","text":"<p>Init producer with specific dependencies and prepare it to work against specified topic(s).</p> <p>Parameters:</p> Name Type Description Default <code>producer</code> <code>AbstractProducer</code> <p>Producer implementation to send messages.</p> required <code>schema_registry</code> <code>Optional[AbstractSchemaRegistry]</code> <p>Schema registry client.</p> required <code>header_packer</code> <code>Optional[HeaderPacker]</code> <p>Callable to form binary headers.</p> required <code>serializer</code> <code>Optional[AbstractSerializer]</code> <p>Common message serializer for the key and value. If specific value_deserializer/key_deserializer defined, it will be used instead.</p> <code>None</code> <code>store</code> <code>Optional[AbstractDescriptionStore]</code> <p>Specific store to provide schema text extraction from schema description.</p> <code>None</code> <code>mapping</code> <code>Optional[Dict[TopicName, MessageDescription]]</code> <p>Per-topic definition of value and/or key schema descriptions.</p> <code>None</code> <code>value_serializer</code> <code>Optional[AbstractSerializer]</code> <p>Message serializer for value, if set.</p> <code>None</code> <code>key_serializer</code> <code>Optional[AbstractSerializer]</code> <p>Message serializer for the key, if set.</p> <code>None</code> <code>protocol_id</code> <code>int</code> <p>Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)</p> <code>1</code> <code>lazy</code> <code>bool</code> <p>If True, defer schema registry publication, otherwise schema will be registered before the first message sending.</p> <code>False</code> Source code in <code>wunderkafka/producers/constructor.py</code> <pre><code>def __init__(  # noqa: WPS211  # ToDo (tribunsky.kir): rethink building of producer.\n    #                                                  Maybe single Callable (like in kafka-python) much better.\n    self,\n    producer: AbstractProducer,\n    # Some serializers doesn't need SR at all, e.g. StringSerializer.\n    schema_registry: Optional[AbstractSchemaRegistry],\n    # As some serializers doesn't contain magic byte, we do not need to handle the first bytes of a message at all.\n    header_packer: Optional[HeaderPacker],\n    serializer: Optional[AbstractSerializer] = None,\n    store: Optional[AbstractDescriptionStore] = None,\n    # ToDo: switch mapping to something like consumer's TopicSubscription?\n    mapping: Optional[Dict[TopicName, MessageDescription]] = None,\n    value_serializer: Optional[AbstractSerializer] = None,\n    key_serializer: Optional[AbstractSerializer] = None,\n    *,\n    protocol_id: int = 1,\n    lazy: bool = False,\n) -&gt; None:\n    \"\"\"\n    Init producer with specific dependencies and prepare it to work against specified topic(s).\n\n    :param producer:            Producer implementation to send messages.\n    :param schema_registry:     Schema registry client.\n    :param header_packer:       Callable to form binary headers.\n    :param serializer:          Common message serializer for the key and value.\n                                If specific value_deserializer/key_deserializer defined, it will be used instead.\n    :param store:               Specific store to provide schema text extraction from schema description.\n    :param mapping:             Per-topic definition of value and/or key schema descriptions.\n    :param value_serializer:    Message serializer for value, if set.\n    :param key_serializer:      Message serializer for the key, if set.\n    :param protocol_id:         Protocol id for producer (1 - Cloudera, 0 - Confluent, etc.)\n    :param lazy:                If True,\n                                defer schema registry publication, otherwise schema will be registered\n                                before the first message sending.\n    \"\"\"\n    self._mapping = mapping or {}\n\n    # ToDo (tribunsky.kir): look like wrong place. Maybe it's better to highlight an entity\n    #                       of Schema which may be checked or not. Then input mapping may be eliminated (or not).\n    self._checked: Dict[Tuple[str, str, bool], SRMeta] = {}\n\n    self._store = store\n    self._sr = schema_registry\n    self._serializer = serializer\n    self._producer = producer\n    self._header_packer = header_packer\n    self._protocol_id = protocol_id\n\n    chosen_value_serializer = value_serializer if value_serializer else serializer\n    if chosen_value_serializer is None:\n        msg = 'Value serializer is not specified, should be passed via value_serializer or serializer at least.'\n        raise ValueError(msg)\n    self._value_serializer = chosen_value_serializer\n\n    chosen_key_serializer = key_serializer if value_serializer else serializer\n    if chosen_key_serializer is None:\n        msg = 'Key serializer is not specified, should be passed via key_serializer or serializer at least.'\n        raise ValueError(msg)\n    self._key_serializer = chosen_key_serializer\n\n    for topic, description in self._mapping.items():\n        if isinstance(description, (tuple, list)):\n            msg_value, msg_key = description\n            self.set_target_topic(topic, msg_value, msg_key, lazy=lazy)\n        else:\n            self.set_target_topic(topic, description, lazy=lazy)\n</code></pre>"},{"location":"pages/advanced_usage/","title":"Advanced Examples","text":"<p>As you've just seen in <code>quickstart</code>, we couldn't eliminate all the boilerplate which is needed to start consumer/producer with ease.</p> <p>So let's see, what we can do here.</p>"},{"location":"pages/advanced_usage/#redefining-configs","title":"Redefining configs","text":"<p>So yes, you may still consider that you need something like this in your code base:</p> <pre><code>import os\nfrom functools import partial\n\nfrom pydantic import field_validator, Field\nfrom wunderkafka.time import now\nfrom wunderkafka import SRConfig, ConsumerConfig, SecurityProtocol, AvroConsumer\n\n\n# If you are a fan of 12 factors, you may want to config via env variables\nclass OverridenSRConfig(SRConfig):\n    url: str = Field(alias='SCHEMA_REGISTRY_URL')\n\n    @field_validator('sasl_username')\n    @classmethod\n    def from_env(cls, v) -&gt; str:\n        # And to use 'native' kerberos envs\n        return '{0}@{1}'.format(os.environ.get('KRB5_USER'), os.environ.get('KRB5_REALM'))\n\n\n# Or you want to override some defaults by default (pun intended)\nclass OverridenConfig(ConsumerConfig):\n    # Consumer which do not commit messages automatically\n    enable_auto_commit: bool = False\n    # And knows nothing after restart due to new gid.\n    group_id: str = 'wunderkafka-{0}'.format(now())\n    # More 12 factors\n    bootstrap_servers: str = Field(env='BOOTSTRAP_SERVER')\n    security_protocol: SecurityProtocol = SecurityProtocol.sasl_ssl\n    sasl_kerberos_kinit_cmd: str = ''\n    sr: SRConfig = OverridenSRConfig()\n\n    @field_validator('sasl_kerberos_kinit_cmd')\n    @classmethod\n    def format_keytab(cls, v) -&gt; str:\n        if not v:\n            return 'kinit {0}@{1} -k -t {0}.keytab'.format(os.environ.get('KRB5_USER'), os.environ.get('KRB5_REALM'))\n        # Still allowing to set it manually\n        return str(v)\n\n\n# After this, you can `partial` your own Producer/Consumer, something like...\nMyConsumer = partial(AvroConsumer, config=OverridenConfig())\n</code></pre> <p>And all this effort is needed to end up with something like:</p> <pre><code>import datetime\n\nfrom my_module import MyConsumer\n\n\nif __name__ == '__main__':\n    consumer = MyConsumer()\n    consumer.subscribe(['my_topic'], with_timedelta=datetime.timedelta(minutes=2))\n    while True:\n        msgs = consumer.consume()\n        ...\n</code></pre> <p>Isn't that simple?</p>"},{"location":"pages/advanced_usage/#yet-another-framework","title":"Yet Another Framework?","text":"<p>Wunderkafka is written with pluggability in mind (and of course assumptions about what may be needed will be broken), so in its core is a little too general. It is intended to provide relatively simple way to rebuild your own pipeline with minimum effort.</p> <pre><code>from typing import Optional\n\nfrom wunderkafka.consumers.bytes import BytesConsumer\nfrom wunderkafka.schema_registry import ClouderaSRClient\nfrom wunderkafka.hotfixes.watchdog import check_watchdog\nfrom wunderkafka.serdes.headers import ConfluentClouderaHeadersHandler\nfrom wunderkafka.consumers.constructor import HighLevelDeserializingConsumer\nfrom wunderkafka.schema_registry.cache import SimpleCache\nfrom wunderkafka.schema_registry.transport import KerberizableHTTPClient\nfrom wunderkafka.serdes.avro.deserializers import FastAvroDeserializer\n\n\ndef MyAvroConsumer(\n    config: Optional[OverridenConfig] = None,\n) -&gt; HighLevelDeserializingConsumer:\n    config = config or OverridenConfig()\n    config, watchdog = check_watchdog(config)\n    return HighLevelDeserializingConsumer(\n        consumer=BytesConsumer(config, watchdog),\n        schema_registry=ClouderaSRClient(KerberizableHTTPClient(config.sr), SimpleCache()),\n        headers_handler=ConfluentClouderaHeadersHandler().parse,\n        deserializer=FastAvroDeserializer(),\n    )\n</code></pre> <p>Looks java'sh? I know, forgive me that.</p> <p>If you like to use inheritance, there is nice examples in <code>wunderkafka.factories</code> to start with. Or check <code>advanced_API</code> (currently to-be-done).</p> <p>As you can see it is pretty straightforward and you can write your own containers for schema handling or (de)serialization, still running above performant <code>librdkafka</code>.</p> <p>It's also simple enough to redefine every part of the (de)serializing pipeline with specific implementation. For example, if there is need to keep message's schema in message itself, it is possible to define stub instead of schema registry and write own header (un)packer.</p>"},{"location":"pages/errata/","title":"Errata","text":""},{"location":"pages/errata/#kerberos-thread","title":"Kerberos Thread","text":""},{"location":"pages/errata/#version-librdkafka-170","title":"Version librdkafka &lt; 1.7.0","text":""},{"location":"pages/errata/#introduction","title":"Introduction","text":"<p>Prior to version 1.7.0, updating kinit via librdkafka could result in a lockup. Because of this, when using a version less than 1.7.0, by default used our thread to update kerberos tickets.</p>"},{"location":"pages/errata/#standard-behavior","title":"Standard behavior","text":"<p>By default, 1 thread is raised to update all tickets and the timeout for all updates is 60 seconds.</p> <p>To set timeout manually you need to</p> <pre><code>from wunderkafka.hotfixes.watchdog import KrbWatchDog\n\nif __name__ == '__main__':\n    krb = KrbWatchDog()\n    krb.krb_timeout = 10  # complete\n    krb.krb_timeout = 'something'  # raised TypeError\n</code></pre>"},{"location":"pages/errata/#bottleneck","title":"Bottleneck","text":"<p>Updating of kerberos tickets is done sequentially one by one.</p>"},{"location":"pages/install/","title":"Installation","text":""},{"location":"pages/install/#default","title":"Default","text":"<pre><code>pip install wunderkafka\n</code></pre>"},{"location":"pages/install/#sasl-kerberosgssapi-support","title":"SASL Kerberos/GSSAPI support","text":"<pre><code>pip install wunderkafka --no-binary confluent-kafka\n</code></pre>"},{"location":"pages/install/#pre-built-docker-images","title":"Pre-built docker images","text":"<pre><code>to-be-done\n</code></pre>"},{"location":"pages/quickstart/","title":"Quickstart","text":"<p>Talk is cheap. Show me the code.</p>          \u2014Linus Torvalds"},{"location":"pages/quickstart/#consumer","title":"Consumer","text":"<pre><code>import datetime\n\nfrom wunderkafka import BytesConsumer, ConsumerConfig, SecurityProtocol\n\nif __name__ == '__main__':\n    # Config is still pretty long, it is an essential complexity.\n    # But in minimal setup you only need to specify just `group_id`\n    # and `bootstrap_servers`\n    config = ConsumerConfig(\n        enable_auto_commit=False,\n        group_id='my_group',\n        bootstrap_servers='kafka-broker-01.my_domain.com:9093',\n        security_protocol=SecurityProtocol.sasl_ssl,\n        sasl_kerberos_kinit_cmd='kinit my_user@my_realm.com -k -t my_user.keytab',\n    )\n\n    consumer = BytesConsumer(config)\n    # topic subscription by different timelines is now oneliner without\n    # much boilerplate.\n    consumer.subscribe(['my_topic'], with_timedelta=datetime.timedelta(hours=10))\n\n    while True:\n        msgs = consumer.batch_poll()\n        print(\n            'Consumed: {0}, errors: {1}'.format(\n                len(msgs), len([msg for msg in msgs if msg.error()])\n            )\n        )\n</code></pre> <p>What you can (and can't see here):</p> <ul> <li>as you probably already guessed, we may need all that serious clumsy     security stuff</li> <li>by the way, config is powered with     pydantic. So you can see all     configuration parameters for supported <code>librdkafka</code> version just in     your IDE/text editor. No more searching with <code>Ctrl + F</code> through     <code>CONFIGURATION.md</code> and dictionaries.</li> <li>you may subscribe to multiple topics via single timestamp or     timedelta (per topic definition is also supported)</li> <li>by default, you don't need to close consumer manually (though, you     can still do it). <code>atexit</code> is used.</li> </ul> <p>More differences are on the way</p>"},{"location":"pages/quickstart/#avroconsumer","title":"AvroConsumer","text":"<pre><code>from wunderkafka import AvroConsumer, ConsumerConfig, SRConfig, SecurityProtocol\n\nBROKERS_ADDRESSES = 'kafka-broker-01.my_domain.com'\nSCHEMA_REGISTRY_URL = 'https://schema-registry.my_domain.com'\n\nif __name__ == '__main__':\n    config = ConsumerConfig(\n        enable_auto_commit=False,\n        group_id='my_group',\n        bootstrap_servers=BROKERS_ADDRESSES,\n        security_protocol=SecurityProtocol.sasl_ssl,\n        sasl_kerberos_kinit_cmd='kinit my_user@my_real.com -k -t my_user.keytab',\n        sr=SRConfig(url=SCHEMA_REGISTRY_URL, sasl_username='my_user@my_real.com'),\n    )\n\n    c = AvroConsumer(config)\n    c.subscribe(['my_topic'], from_beginning=True)\n\n    while True:\n        msgs = c.consume(timeout=10.0, ignore_keys=True)\n        print(len(msgs))\n</code></pre> <ul> <li>keys may be ignored, which is totally optional, but may be useful.</li> <li>the main advantage here is that messages may be consumed with a     batch even with avro deserialization, despite of the original API.     It saves time. Really.</li> </ul> <p>Multiple subscriptions example:</p> <p>Let's move on.</p>"},{"location":"pages/quickstart/#avroproducer","title":"AvroProducer","text":"<p>Let's skip raw producer, as we can see all benefits in AvroProducer either.</p> <pre><code>from wunderkafka import AvroProducer, ProducerConfig, SRConfig, SecurityProtocol\n\nBROKERS_ADDRESSES = 'kafka-broker-01.my_domain.com'\nSCHEMA_REGISTRY_URL = 'https://schema-registry.my_domain.com'\n\nvalue_schema = \"\"\"\n{\n   \"namespace\": \"my.test\",\n   \"name\": \"value\",\n   \"type\": \"record\",\n   \"fields\" : [\n     {\n       \"name\" : \"name\",\n       \"type\" : \"string\"\n     }\n   ]\n}\n\"\"\"\nkey_schema = \"\"\"\n{\n   \"namespace\": \"my.test\",\n   \"name\": \"key\",\n   \"type\": \"record\",\n   \"fields\" : [\n     {\n       \"name\" : \"name\",\n       \"type\" : \"string\"\n     }\n   ]\n}\n\"\"\"\n\nif __name__ == '__main__':\n    config = ProducerConfig(\n        bootstrap_servers=BROKERS_ADDRESSES,\n        security_protocol=SecurityProtocol.sasl_ssl,\n        sasl_kerberos_kinit_cmd='kinit my_user@my_real.com -k -t my_user.keytab',\n        sr=SRConfig(url=SCHEMA_REGISTRY_URL, sasl_username='my_user@my_real.com'),\n    )\n\n    topic = 'test_test_test'\n    producer = AvroProducer({topic: (value_schema, key_schema)}, config)\n    producer.send_message(topic, {\"name\": \"Value\"}, {\"name\": \"Key\"}, blocking=True)\n</code></pre> <ul> <li>instead of producing message we are thinking in terms of sending     message. No big deal as original <code>produce()</code> is still under the     hood, but we automatically use <code>poll()</code> for asynchronous     communication and <code>flush()</code> to await that message is sent. This     behaviour is hidden by <code>blocking</code> which is <code>False</code> by default.</li> <li>by the way, <code>atexit</code> is also used here: producer will try to     <code>flush()</code>. Nothing is guaranteed if something sudden will happen     with process, but manual close is also in danger in that case.</li> <li>less boilerplate with text schemas. You may also load it simply from     files (via specific \"store\"), but wait for a minute, you may     won't want to use them.</li> </ul>"},{"location":"pages/quickstart/#avromodelproducer","title":"AvroModelProducer","text":"<p>What if you don't believe in everything-as-code and want more dynamics? Let's consider the next few lines:</p> <pre><code>from pydantic import BaseModel, Field\nfrom wunderkafka import AvroModelProducer, ProducerConfig, SRConfig, SecurityProtocol\nfrom wunderkafka.time import now\n\nBROKERS_ADDRESSES = 'kafka-broker-01.my_domain.com'\nSCHEMA_REGISTRY_URL = 'https://schema-registry.my_domain.com'\n\n\nclass SomeEvent(BaseModel):\n    name: str = 'test'\n    ts: int = Field(default_factory=now)\n\n    class Meta:\n        name = 'my.test'\n\n\nif __name__ == '__main__':\n    config = ProducerConfig(\n        bootstrap_servers=BROKERS_ADDRESSES,\n        security_protocol=SecurityProtocol.sasl_ssl,\n        sasl_kerberos_kinit_cmd='kinit my_user@my_real.com -k -t my_user.keytab',\n        sr=SRConfig(url=SCHEMA_REGISTRY_URL, sasl_username='my_user@my_real.com'),\n    )\n\n    topic = 'test_test_test'\n\n    # No key, that's just an example\n    producer = AvroModelProducer({topic: SomeEvent}, config)\n    producer.send_message(topic, SomeEvent(), blocking=True)\n</code></pre> <ul> <li>just like the previous example, but the schema derived from the     model itself. <code>dataclasses</code> are also supported, thanks to     dataclasses-avroschema!</li> </ul>"},{"location":"pages/quickstart/#conclusion","title":"Conclusion","text":"<p>This is a simple API for \"daily\" usage.</p> <p>You still can use original rich API of confluent-kafka if needed, but from now you have some fast track.</p>"},{"location":"pages/rationale/","title":"Rationale","text":""},{"location":"pages/rationale/#rationale","title":"Rationale","text":"<pre><code>Das ist wunderbar!\n</code></pre>"},{"location":"pages/rationale/#what-we-are-about","title":"What we are about?","text":"<ul> <li>Cloudera installation with its own     schema registry</li> <li>Apache Avro\u2122 is used</li> <li>Installation requires features which are fully supported by     librdkafka, but not     bundled in confluent-kafka python wheel</li> <li>Constant need to use producers and consumer, but without one-screen     boilerplate</li> <li>Frequent need to consume not purely events, but fairly recent     events</li> <li>Frequent need to handle a large number of events</li> </ul> <p>So, that's it.</p> <p>If you suffer from the same problems, you may don't need to reinvent your own wheel, you can try ours.</p>"},{"location":"pages/rationale/#what-about-other-projects","title":"What about other projects?","text":"<p>Corresponding to ASF wiki there are plenty of python clients.</p> <ul> <li>confluent-kafka is a     de-facto standard, but doesn't work out-of-the-box for us, as     mentioned above</li> <li>Kafka Python is awesome, but     not as performant as confluent-kafka</li> <li>pykafka here and     here looks unmaintained: has     been archived</li> <li>pykafkap has only     producer and looks unmaintained: no updates since 2014</li> <li>brod is not maintained in favor     of Kafka Python.</li> </ul>"},{"location":"pages/rationale/#whats-next","title":"What's next?","text":"<p>For now, it's a homebrew, so it lacks some of the features which may be useful outside of our use-cases.</p> <p>ToDo:</p> <ul> <li>add configurations for multiple versions of librdkafka</li> <li>check against confluent installation</li> <li>add <code>async</code>/<code>await</code> syntax</li> <li>parallelize (de)serialization on CPU</li> <li>add distributed lock on producers</li> <li>add on-the-fly model derivation to consumer</li> <li>???</li> </ul>"}]}